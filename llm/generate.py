from __future__ import annotations
import zipfile
from yandex_cloud_ml_sdk import YCloudML
import os
from pathlib import Path
import requests, json
from pathlib import Path
import shutil


API_KEY = os.getenv("api_key")
FOLDER_ID = os.getenv("folder")
URL = "https://llm.api.cloud.yandex.net/foundationModels/v1/completion"


model_uri = f"gpt://{FOLDER_ID}/yandexgpt-32k/latest"
MODEL_URI = f"gpt://{FOLDER_ID}/yandexgpt-32k/latest"


output_dir = "/content/generated_docs"
os.makedirs(output_dir, exist_ok=True)


def unzip_bytes_to_dir(zip_bytes: bytes, extract_path: str) -> None:
    """
    Extracts zip archive from bytes to specified path
    """
    temp_zip = Path(extract_path) / "temp.zip"
    temp_zip.write_bytes(zip_bytes)

    with zipfile.ZipFile(temp_zip, "r") as zip_ref:
        zip_ref.extractall(extract_path)

    temp_zip.unlink()  # Remove temporary zip file


def collect_python_files(repo_path: str) -> list[str]:
    """
    —Å–æ–±–∏—Ä–∞–µ—Ç –≤—Å–µ .py-—Ñ–∞–π–ª—ã
    """
    py_files = []
    for root, _, files in os.walk(repo_path):
        for fn in files:
            if fn.endswith(".py"):
                py_files.append(os.path.join(root, fn))
    return py_files


files = collect_python_files("/content/clean-architecture")
print("–ù–∞–π–¥–µ–Ω–æ .py-—Ñ–∞–π–ª–æ–≤:", len(files))


system_prompt = "–¢—ã ‚Äî —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ –ø—Ä–æ–≥—Ä–∞–º–º–Ω–æ–≥–æ –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è. –î–æ–ª–∂–µ–Ω –≤ –æ—Ç–≤–µ—Ç –Ω–∞ –ø—Ä–∏—Å–ª–∞–Ω–Ω—ã–π –∫–æ–¥ –ø—Ä–∏—Å—ã–ª–∞—Ç—å –∫—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ —Ç–æ–≥–æ, –∫–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç —ç—Ç–∞ –ø—Ä–æ–≥—Ä–∞–º–º–∞. –ù–µ –≤—Å—Ç–∞–≤–ª—è–π –Ω–∏–∫–∞–∫–æ–π –∫–æ–¥ –≤ —Å–≤–æ–π –æ—Ç–≤–µ—Ç. –¢–æ–ª—å–∫–æ –∫—Ä–∞—Ç–∫–æ–µ —Ç–µ–∫—Å—Ç–æ–≤–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ —Ä–∞–±–æ—Ç—ã"

headers = {
    "Authorization": f"Api-Key {API_KEY}",
    "x-folder-id": FOLDER_ID,
    "Content-Type": "application/json",
}


def generate_doc_for_file(filepath: str) -> str:
    code = Path(filepath).read_text(encoding="utf-8")
    if not code.strip():
        print(f"Skipping empty file: {filepath}")
        return ""

    payload = {
        "modelUri": model_uri,
        "completionOptions": {"stream": False, "temperature": 0.3, "maxTokens": 1500},
        "messages": [
            {"role": "system", "text": system_prompt},
            {"role": "user", "text": code},
        ],
    }

    print(filepath)

    r = requests.post(URL, headers=headers, json=payload, timeout=120)
    if r.status_code != 200:
        print(f"Status code: {r.status_code}")
        print(f"Response content: {r.text}")
    r.raise_for_status()
    return r.json()["result"]["alternatives"][0]["message"]["text"]

def zip_docs() -> Path:
    """
    Creates a zip archive of the generated documentation
    Returns path to the created zip file
    """
    docs_path = Path("/content/generated_docs")
    zip_path = Path("/content/docs.zip")

    with zipfile.ZipFile(zip_path, "w", zipfile.ZIP_DEFLATED) as zipf:
        for file in docs_path.rglob("*"):
            if file.is_file():
                zipf.write(file, file.relative_to(docs_path))

    return zip_path


def generate_docs(zip_file: bytes) -> Path:
    """
    Generates documentation for the provided zip file
    Returns path to generated zip file
    """
    # Create paths using Path objects
    # Setup paths
    temp_dir = Path("/content/temp_repo")
    output_dir = Path("/content/generated_docs")
    

    # Create directories
    temp_dir.mkdir(parents=True, exist_ok=True)
    output_dir.mkdir(parents=True, exist_ok=True)

    # Clean up existing content
    if temp_dir.exists():
        shutil.rmtree(temp_dir)
    temp_dir.mkdir(parents=True)

    # Unzip the received bytes
    unzip_bytes_to_dir(zip_file, str(temp_dir))

    # Collect Python files
    py_files = collect_python_files(str(temp_dir))
    print(f"–ë—É–¥–µ–º –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å {len(py_files)} —Ñ–∞–π–ª–æ–≤‚Ä¶")

    for path in py_files:
        try:
            print("–û–±—Ä–∞–±–æ—Ç–∫–∞:", path)
            doc = generate_doc_for_file(path)
            md_name = Path(path).relative_to(temp_dir).with_suffix(".md")
            out_path = output_dir / md_name
            out_path.parent.mkdir(parents=True, exist_ok=True)
            out_path.write_text(doc, encoding="utf-8")
            print("–°–æ—Ö—Ä–∞–Ω–µ–Ω–æ:", out_path)
        except Exception as e:
            print("–û—à–∏–±–∫–∞ –¥–ª—è", path, ":", e)


def generate_module_docs():

    docs_root = Path("/content/generated_docs")
    HEADERS = headers  # Reuse headers defined above

    MAX_CHARS = 10000  # –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞ –∑–∞–ø—Ä–æ—Å–∞

    def merge_batch(mod_name: str, files_batch: list[tuple[str, str]]) -> str:
        joined = "\n\n".join(f"### {Path(path).name}\n{text}" for path, text in files_batch)
        prompt = (
            f"–î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ —Ñ–∞–π–ª–æ–≤ –º–æ–¥—É–ª—è `{mod_name}`. –°–ª–µ–π –∏—Ö –≤ –µ–¥–∏–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ. "
            f"–û–ø–∏—à–∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É, –Ω–∞–∑–Ω–∞—á–µ–Ω–∏–µ, –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –∏ use-cases. Markdown-—Ñ–æ—Ä–º–∞—Ç.\n\n{joined}"
        )
        payload = {
            "modelUri": MODEL_URI,
            "completionOptions": {"stream": False, "temperature": 0.2, "maxTokens": 1200},
            "messages": [
                {
                    "role": "system",
                    "text": "–¢—ã ‚Äî AI-–¥–æ–∫—É–º–µ–Ω—Ç–æ–≤–µ–¥. –û–±—ä–µ–¥–∏–Ω—è–π –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ —Ñ–∞–π–ª–æ–≤ –≤ –æ–ø–∏—Å–∞–Ω–∏–µ –º–æ–¥—É–ª—è.",
                },
                {"role": "user", "text": prompt},
            ],
        }
        response = requests.post(URL, headers=HEADERS, json=payload, timeout=120)
        response.raise_for_status()
        return response.json()["result"]["alternatives"][0]["message"]["text"]

    def split_into_batches(md_files, max_chars=MAX_CHARS):
        batches, batch, size = [], [], 0
        for f in md_files:
            length = len(f[1])
            if size + length > max_chars and batch:
                batches.append(batch)
                batch, size = [], 0
            batch.append(f)
            size += length
        if batch:
            batches.append(batch)
        return batches

    # –û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª —Å –±–∞—Ç—á–∞–º–∏
    for mod_dir in docs_root.iterdir():
        if not mod_dir.is_dir():
            continue
        mod_name = mod_dir.name
        print(f"\nüì¶ –ú–æ–¥—É–ª—å: {mod_name}")

        md_files = [(str(f), f.read_text(encoding="utf-8")) for f in mod_dir.rglob("*.md")]
        if not md_files:
            print(f"–ù–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –º–æ–¥—É–ª—è {mod_name}")
            continue

        batches = split_into_batches(md_files)
        print(f"–†–∞–∑–±–∏–ª–∏ –Ω–∞ {len(batches)} –±–∞—Ç—á–µ–π.")

        module_docs = []
        for i, batch in enumerate(batches, 1):
            print(f"üî∏ –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –±–∞—Ç—á {i}/{len(batches)} (—Ñ–∞–π–ª–æ–≤: {len(batch)})")
            try:
                part_doc = merge_batch(mod_name, batch)
                module_docs.append(part_doc)
            except requests.exceptions.HTTPError as e:
                print(f"–û—à–∏–±–∫–∞ –Ω–∞ –±–∞—Ç—á–µ {i} –º–æ–¥—É–ª—è '{mod_name}': {e}")
                continue

        final_doc = "\n\n".join(module_docs)

        #  –¢–µ–ø–µ—Ä—å —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤–Ω—É—Ç—Ä–∏ –ø–∞–ø–∫–∏ –º–æ–¥—É–ª—è:
        out_path = mod_dir / f"{mod_name}_module.md"
        out_path.write_text(final_doc, encoding="utf-8")
        print(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤ –ø–∞–ø–∫–µ –º–æ–¥—É–ª—è: {out_path}")


def generate_overview_docs():
    docs_root = Path("/content/generated_docs")
    output_path = docs_root / "project_overview.md"

    from urllib.parse import quote

    project_root_path = Path("/content")

    module_entries = []
    module_docs = []

    for f in docs_root.rglob("*_module.md"):
        mod_name = f.stem.replace("_module", "")
        rel_doc_path = f.relative_to(docs_root)

        source_mod_path = project_root_path / rel_doc_path.parent
        source_mod_path_str = str(source_mod_path).replace("\\", "/")

        md_link = f"[{mod_name}]({quote(source_mod_path_str)})"

        module_entries.append(f"- {md_link}  \n  üìÅ –ü—É—Ç—å: `{source_mod_path_str}`")

        text = f.read_text(encoding="utf-8")
        module_docs.append((mod_name, text))

    module_summary_md = "### –û—Å–Ω–æ–≤–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –∏ –º–æ–¥—É–ª–∏\n\n" + "\n".join(module_entries)

    joined = "\n\n".join(f"## {mod_name}\n\n{text[:2000]}" for mod_name, text in module_docs)

    prompt = (
        "–°–æ–∑–¥–∞–π –∫—Ä–∞—Ç–∫–æ–µ –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ –µ–≥–æ –º–æ–¥—É–ª–µ–π. "
        "–í–∫–ª—é—á–∏ —Å–ª–µ–¥—É—é—â–∏–µ —Ä–∞–∑–¥–µ–ª—ã:\n"
        "- –ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ —Å–∏—Å—Ç–µ–º—ã\n"
        "- –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã–π —Å—Ç–∏–ª—å –∏ –ø—Ä–∏–Ω—Ü–∏–ø—ã\n"
        "- –û—Å–Ω–æ–≤–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –∏ –º–æ–¥—É–ª–∏ (—Å –Ω–∞–∑–≤–∞–Ω–∏—è–º–∏, —Å—Å—ã–ª–∫–∞–º–∏ –∏ –ø—É—Ç—è–º–∏)\n"
        "- –í–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ –∏ –∫–ª—é—á–µ–≤—ã–µ —Å—Ü–µ–Ω–∞—Ä–∏–∏ (use-cases)\n\n"
        "–û—Ç–≤–µ—Ç ‚Äî –≤ Markdown.\n\n"
        f"{module_summary_md}\n\n"
        f"{joined}"
    )

    payload = {
        "modelUri": MODEL_URI,
        "completionOptions": {"stream": False, "temperature": 0.2, "maxTokens": 2000},
        "messages": [
            {"role": "system", "text": "–¢—ã ‚Äî AI-–∞—Ä—Ö–∏—Ç–µ–∫—Ç–æ—Ä. –°–æ—Å—Ç–∞–≤—å –∫—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–æ–¥—É–ª—å–Ω–æ–π –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏."},
            {"role": "user", "text": prompt}
        ]
    }
    HEADERS = headers

    response = requests.post(URL, headers=HEADERS, json=payload, timeout=120)
    response.raise_for_status()
    overview = response.json()["result"]["alternatives"][0]["message"]["text"]

    output_path.write_text(overview, encoding="utf-8")
    print(f"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {output_path}")
    print("\n---\n", overview[:1000], "\n...")
